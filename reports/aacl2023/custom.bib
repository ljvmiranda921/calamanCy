% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@inproceedings{Goodfellow2013MaxoutN,
  title     = {Maxout Networks},
  author    = {Goodfellow, Ian and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {1319--1327},
  year      = {2013},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  volume    = {28},
  number    = {3},
  series    = {Proceedings of Machine Learning Research},
  address   = {Atlanta, Georgia, USA},
  month     = {17--19 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v28/goodfellow13.pdf},
  url       = {https://proceedings.mlr.press/v28/goodfellow13.html},
  abstract  = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}


@inproceedings{Lample2016NeuralAF,
  title     = {Neural Architectures for Named Entity Recognition},
  author    = {Lample, Guillaume  and
               Ballesteros, Miguel  and
               Subramanian, Sandeep  and
               Kawakami, Kazuya  and
               Dyer, Chris},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N16-1030},
  doi       = {10.18653/v1/N16-1030},
  pages     = {260--270}
}

@inproceedings{Kroeger1992PhraseSA,
  title  = {{Phrase Structure and Grammatical Relations in Tagalog}},
  author = {Paul R. Kroeger},
  year   = {1992},
  url    = {https://api.semanticscholar.org/CorpusID:60973484}
}

@online{DatabricksBlog2023DollyV2,
  author  = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title   = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year    = {2023},
  url     = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate = {2023-06-30}
}

@article{Falcon40b,
  title  = {{Falcon-40B}: an open large language model with state-of-the-art performance},
  author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year   = {2023}
}

@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title  = {{OpenLLaMA: An Open Reproduction of LLaMA}},
  month  = May,
  year   = 2023,
  url    = {https://github.com/openlm-research/open_llama}
}

@inproceedings{Strassel2016LORELEILP,
  title     = {{LORELEI} Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages},
  author    = {Strassel, Stephanie  and
               Tracey, Jennifer},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
  month     = may,
  year      = {2016},
  address   = {Portoro{\v{z}}, Slovenia},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/L16-1521},
  pages     = {3273--3280},
  abstract  = {In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA{'}s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.}
}

@inproceedings{Papay2020DissectingSI,
  title     = {Dissecting Span Identification Tasks with Performance Prediction},
  author    = {Papay, Sean  and
               Klinger, Roman  and
               Pad{\'o}, Sebastian},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.396},
  doi       = {10.18653/v1/2020.emnlp-main.396},
  pages     = {4881--4895},
  abstract  = {Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks{'} properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.}
}

@inproceedings{Doddington2004TheAC,
  title     = {The Automatic Content Extraction ({ACE}) Program {--} Tasks, Data, and Evaluation},
  author    = {Doddington, George  and
               Mitchell, Alexis  and
               Przybocki, Mark  and
               Ramshaw, Lance  and
               Strassel, Stephanie  and
               Weischedel, Ralph},
  booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)},
  month     = may,
  year      = {2004},
  address   = {Lisbon, Portugal},
  publisher = {European Language Resources Association (ELRA)},
  url       = {http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf}
}

@article{Schachter1973TagalogRG,
  title   = {{Tagalog reference grammar}},
  author  = {Paul Schachter and Fe T. Otanes},
  journal = {The Journal of Asian Studies},
  year    = {1973},
  volume  = {32},
  pages   = {760 - 761},
  url     = {https://api.semanticscholar.org/CorpusID:162393304}
}

@inproceedings{Pan2017CrosslingualNT,
  title     = {Cross-lingual Name Tagging and Linking for 282 Languages},
  author    = {Pan, Xiaoman  and
               Zhang, Boliang  and
               May, Jonathan  and
               Nothman, Joel  and
               Knight, Kevin  and
               Ji, Heng},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P17-1178},
  doi       = {10.18653/v1/P17-1178},
  pages     = {1946--1958},
  abstract  = {The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.}
}

@article{Costiniano2022CustomCG,
  title   = {{Custom Coarse Grained Named Entity Recognition for Filipino Storytelling Data Using Uncased Transformer Models}},
  author  = {Sherwyne Costiniano and Rose Ann Mae Santos and Julius Simon Mendoza and Allen Jay Gale},
  journal = {SSRN Electronic Journal},
  year    = {2022},
  url     = {https://api.semanticscholar.org/CorpusID:255117774}
}


@misc{Lewis2009EthnologueL,
  title        = {{Ethnologue: languages of the world}},
  author       = {Paul M. A. Lewis},
  howpublished = {\url{https://ethnologue.com/language/tgl}},
  year         = {2009},
  note         = {Accessed: June 2023}
}


@misc{Reiter2017HT,
  title        = {{How to Develop Annotation Guidelines}},
  author       = {Nils Reiter},
  howpublished = {\url{https://sharedtasksinthedh.github.io/2017/10/01/howto-annotation/}},
  year         = {2017},
  note         = {Accessed: June 2023}
}


@inproceedings{Pires2019HowMI,
  title     = {{How Multilingual is Multilingual BERT?}},
  author    = {Telmo Pires and Eva Schlinger and Dan Garrette},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{Devlin2019BERTPO,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{Conneau2019UnsupervisedCR,
  title     = {{Unsupervised Cross-lingual Representation Learning at Scale}},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}


@inproceedings{Cruz2021ImprovingLL,
  title     = {{Improving Large-scale Language Models and Resources for {F}ilipino}},
  author    = {Cruz, Jan Christian Blaise  and
               Cheng, Charibeth},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.703},
  pages     = {6548--6555},
  abstract  = {In this paper, we improve on existing language resources for the low-resource Filipino language in two ways. First, we outline the construction of the TLUnified dataset, a large-scale pretraining corpus that serves as an improvement over smaller existing pretraining datasets for the language in terms of scale and topic variety. Second, we pretrain new Transformer language models following the RoBERTa pretraining technique to supplant existing models trained with small corpora. Our new RoBERTa models show significant improvements over existing Filipino models in three benchmark datasets with an average gain of 4.47{\%} test accuracy across three classification tasks with varying difficulty.}
}

@inproceedings{Aquino2020ParsingIT,
  title     = {{Parsing in the absence of related languages: Evaluating low-resource dependency parsers on Tagalog}},
  author    = {Angelina A. Aquino and Franz A. de Leon},
  booktitle = {Universal Dependencies Workshop},
  year      = {2020}
}

@inproceedings{Kondratyuk201975L1,
  title     = {{75 Languages, 1 Model: Parsing Universal Dependencies Universally}},
  author    = {D. Kondratyuk},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2019}
}

@inproceedings{Dehouck2019PhylogenicMD,
  title     = {{Phylogenic Multi-Lingual Dependency Parsing}},
  author    = {Mathieu Dehouck and P. Denis},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  year      = {2019}
}

@article{Enriquez2023DeterminingLF,
  title   = {{Determining Linguistic Features of Hate Speech from 2016 Philippine Election-Related Tweets}},
  author  = {Raphael Christen K. Enriquez and Maria Regina Justina Estuar},
  journal = {2023 International Conference on IT Innovation and Knowledge Discovery (ITIKD)},
  year    = {2023},
  pages   = {1-6}
}

@article{Cabasag2016HatespeechIP,
  title   = {{Hate Speech in Philippine Election-Related Tweets: Automatic Detection and Classification Using Natural Language Processing}},
  author  = {Neil Vicente P. Cabasag and Vicente Raphael C. Chan and Sean Christian Y. Lim and Mark Edward M. Gonzales and Charibeth K. Cheng},
  journal = {Philippine Computing Journal Dedicated Issue on Natural Language Processing},
  year    = {2019},
  pages   = {1-14}
}

@article{Livelo2018IntelligentDI,
  title   = {{Intelligent Dengue Infoveillance Using Gated Recurrent Neural Learning and Cross-Label Frequencies}},
  author  = {Evan Dennison S. Livelo and Charibeth Ko Cheng},
  journal = {2018 IEEE International Conference on Agents (ICA)},
  year    = {2018},
  pages   = {2-7}
}

@article{Honnibal2020Spacy,
  title  = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  doi    = {10.5281/zenodo.1212303},
  year   = {2020}
}

@inproceedings{Enevoldsen2021DaCyAU,
  title     = {{DaCy: A Unified Framework for Danish NLP}},
  author    = {Kenneth C. Enevoldsen and L M Hansen and Kristoffer Laigaard Nielbo},
  booktitle = {Workshop on Computational Humanities Research},
  year      = {2021}
}

@article{Orosz2022HuSpaCyAI,
  title   = {{HuSpaCy: an industrial-strength Hungarian natural language processing toolkit}},
  author  = {Gy{\"o}rgy Orosz and Zsolt Sz{\'a}nt{\'o} and P{\'e}ter Berkecz and Gergo Szab{\'o} and Rich{\'a}rd Farkas},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2201.01956}
}

@article{Eyre2021LaunchingIC,
  title   = {{Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python}},
  author  = {Hannah Eyre and Alec B. Chapman and Kelly S. Peterson and Jianlin Shi and Patrick R. Alba and Makoto M. Jones and Tam{\'a}ra L Box and Scott L Duvall and Olga V Patterson},
  journal = {Proceedings of the AMIA Annual Symposium},
  year    = {2021},
  volume  = {2021},
  pages   = {438-447}
}

@article{Neumann2019ScispaCyFA,
  title   = {{ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing}},
  author  = {Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1902.07669}
}

@article{Wolf2019HuggingFacesTS,
  title   = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
  author  = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1910.03771}
}

@inproceedings{Cruz2020ExploitingNA,
  title     = {{Exploiting News Article Structure for Automatic Corpus Generation of Entailment Datasets}},
  author    = {Jan Christian Blaise Cruz and Jose Kristian Resabal and James Lin and Dan John Velasco and Charibeth Ko Cheng},
  booktitle = {Pacific Rim International Conference on Artificial Intelligence},
  year      = {2020}
}

@inproceedings{OrtizSuarez2019AsynchronousPF,
  title     = {{Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures}},
  author    = {Pedro Ortiz Suarez and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {7th Workshop on the Challenges in the Management of Large Corpora},
  year      = {2019}
}

@inproceedings{Sang2003IntroductionTT,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}

@inproceedings{Sang2002IntroductionTT,
  title     = {Introduction to the {C}o{NLL}-2002 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.},
  booktitle = {{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002)},
  year      = {2002},
  url       = {https://aclanthology.org/W02-2024}
}

@article{Delger2012BuildingGS,
  title   = {{Building Gold Standard Corpora for Medical Natural Language Processing Tasks}},
  author  = {Louise Del{\'e}ger and Qi Li and Todd Lingren and Megan Kaiser and Katalin Moln{\'a}r and Laura Stoutenborough and Michal Kouril and Keith A. Marsolo and Imre Solti},
  journal = {Proceedings of the AMIA Annual Symposium},
  year    = {2012},
  volume  = {2012},
  pages   = {144-53}
}

@article{Bojanowski2016EnrichingWV,
  title     = {{Enriching Word Vectors with Subword Information}},
  author    = {Bojanowski, Piotr  and
               Grave, Edouard  and
               Joulin, Armand  and
               Mikolov, Tomas},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {5},
  year      = {2017},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q17-1010},
  doi       = {10.1162/tacl_a_00051},
  pages     = {135--146},
  abstract  = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}


@inproceedings{Vaswani2017AttentionIA,
  title     = {{Attention is All you Need}},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems},
  year      = {2017}
}

@inproceedings{Jiang2021PretrainedLM,
  title     = {{Pre-trained Language Models for Tagalog with Multi-source Data}},
  author    = {Shengyi Jiang and Yingwen Fu and Xiaotian Lin and Nankai Lin},
  booktitle = {Natural Language Processing and Chinese Computing},
  year      = {2021}
}

@mastersthesis{Samson2018TRG,
  author  = {Stephanie Dawn Samson},
  type    = {Bachelor's Thesis},
  title   = {{A treebank prototype of Tagalog}},
  school  = {University of T{\"u}bingen},
  address = {Germany},
  year    = {2018}
}

@inproceedings{Agic2017CrossLingualPS,
  title     = {{Cross-Lingual Parser Selection for Low-Resource Languages}},
  author    = {Agi\'{c}, \v{Z}eljko},
  booktitle = {Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies},
  pages     = {1-10},
  year      = {2017}
}


@inproceedings{Haspelmath2005WALS,
  title     = {{The World Atlas of Language Structures}},
  author    = {Martin Haspelmath and Matthew Dryer and David Gil and Comrie Bernard},
  booktitle = {Oxford University Press},
  year      = {2005}
}

@article{Miranda2022MultiHE,
  title   = {{Multi hash embeddings in spaCy}},
  author  = {Lester James V. Miranda and \'Akos K\'ad\'ar and Adriane Boyd and Sofie Van Landeghem and Anders S{\o}gaard and Matthew Honnibal},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2212.09255}
}

@inproceedings{Brandsen2020CreatingAD,
  title     = {{Creating a Dataset for Named Entity Recognition in the Archaeology Domain}},
  author    = {Brandsen, Alex  and
               Verberne, Suzan  and
               Wansleeben, Milco  and
               Lambers, Karsten},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.562},
  pages     = {4573--4577},
  abstract  = {In this paper, we present the development of a training dataset for Dutch Named Entity Recognition (NER) in the archaeology domain. This dataset was created as there is a dire need for semantic search within archaeology, in order to allow archaeologists to find structured information in collections of Dutch excavation reports, currently totalling around 60,000 (658 million words) and growing rapidly. To guide this search task, NER is needed. We created rigorous annotation guidelines in an iterative process, then instructed five archaeology students to annotate a number of documents. The resulting dataset contains {\textasciitilde}31k annotations between six entity types (artefact, time period, place, context, species {\&} material). The inter-annotator agreement is 0.95, and when we used this data for machine learning, we observed an increase in F1 score from 0.51 to 0.70 in comparison to a machine learning model trained on a dataset created in prior work. This indicates that the data is of high quality, and can confidently be used to train NER classifiers.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}



@inproceedings{Deleger2012BG,
  title     = {{Building gold standard corpora for medical natural language processing tasks}},
  author    = {Louise Deleger and Qi Li and Todd Lingren and Megan Kaiser and Katalin Molnar and Laura Stoutenborough and Michal Kouril and Keith Marsolo and Imre Solti},
  booktitle = {AMIA Annual Symposium Proceedings},
  year      = {2012},
  pages     = {144-53}
}

@misc{Nivre2020UniversalDV,
  title        = {{Data Release Checklist - Universal Dependencies}},
  author       = {Joakim Nivre and Marie-Catherine de Marneffe and Filip Ginter and Jan Hajivc and Christopher D. Manning and Sampo Pyysalo and Sebastian Schuster and Francis M. Tyers and Daniel Zeman},
  howpublished = {\url{https://universaldependencies.org/release_checklist.html#data-split}},
  year         = {2022},
  note         = {Accessed: June 2023}
}

@inproceedings{Lauscher2020FromZT,
  title     = {From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers},
  author    = {Anne Lauscher and Vinit Ravishankar and Ivan Vulic and Goran Glavas},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2020}
}

@article{Ruder2019ASO,
  title   = {A Survey of Cross-lingual Word Embedding Models},
  author  = {Sebastian Ruder and Ivan Vulic and Anders S{\o}gaard},
  journal = {J. Artif. Intell. Res.},
  year    = {2019},
  volume  = {65},
  pages   = {569-631}
}


@inproceedings{Briakou2023SearchingFN,
  title     = {Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in {P}a{LM}{'}s Translation Capability},
  author    = {Briakou, Eleftheria  and
               Cherry, Colin  and
               Foster, George},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.524},
  doi       = {10.18653/v1/2023.acl-long.524},
  pages     = {9432--9452},
  abstract  = {Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism{---}the unintentional consumption of bilingual signals, including translation examples{---}in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM{'}s out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale.}
}


@inproceedings{Wang2022WhatLM,
  title     = {What Language Model Architecture and Pretraining Objective Works Best for Zero-Shot Generalization?},
  author    = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages     = {22964--22984},
  year      = {2022},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume    = {162},
  series    = {Proceedings of Machine Learning Research},
  month     = {17--23 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v162/wang22u/wang22u.pdf},
  url       = {https://proceedings.mlr.press/v162/wang22u.html},
  abstract  = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 168 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely self-supervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. Code and checkpoints are available at https://github.com/bigscience- workshop/architecture-objective.}
}

@misc{OpenAI2023GPT4,
  title         = {{GPT-4 Technical Report}},
  author        = {OpenAI},
  year          = {2023},
  eprint        = {2303.08774},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}


@misc{Anthropic2023Claude,
  title  = {Model Card and Evaluations for Claude Models},
  author = {Anthropic},
  year   = {2023},
  url    = {https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf}
}


@misc{Command2023Cohere,
  title  = {{Command Model: The AI-Powered Solution for the Enterprise}},
  author = {Cohere},
  year   = {2023},
  url    = {https://cohere.com/models/command}
}


@misc{StableLM2023,
  title  = {{StableLM-Alpha v2}},
  author = {Stability-AI},
  year   = {2023},
  url    = {https://github.com/Stability-AI/StableLM#stablelm-alpha-v2}
}

@misc{Ashok2023PromptNER,
  title         = {PromptNER: Prompting For Named Entity Recognition},
  author        = {Dhananjay Ashok and Zachary C. Lipton},
  year          = {2023},
  eprint        = {2305.15444},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@misc{Wei2023CoT,
  title         = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author        = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
  year          = {2023},
  eprint        = {2201.11903},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{Webson2022DoPB,
  title     = {Do Prompt-Based Models Really Understand the Meaning of Their Prompts?},
  author    = {Webson, Albert  and
               Pavlick, Ellie},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jul,
  year      = {2022},
  address   = {Seattle, United States},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.naacl-main.167},
  doi       = {10.18653/v1/2022.naacl-main.167},
  pages     = {2300--2344},
  abstract  = {Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompts manually written for natural language inference (NLI). We find that models can learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively {``}good{''} prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2021). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models{'} impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans{'} use of task instructions.}
}



@inproceedings{Zhao2021CalibrateBU,
  title     = {Calibrate Before Use: Improving Few-shot Performance of Language Models},
  author    = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages     = {12697--12706},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  volume    = {139},
  series    = {Proceedings of Machine Learning Research},
  month     = {18--24 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf},
  url       = {https://proceedings.mlr.press/v139/zhao21c.html},
  abstract  = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model’s bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2’s accuracy (up to 30.0% absolute) across different choices of the prompt, while also making learning considerably more stable.}
}
