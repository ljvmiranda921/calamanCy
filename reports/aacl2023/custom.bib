% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@inproceedings{Goodfellow2013MaxoutN,
  title     = {Maxout Networks},
  author    = {Ian J. Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron C. Courville and Yoshua Bengio},
  booktitle = {International Conference on Machine Learning},
  year      = {2013},
  url       = {https://api.semanticscholar.org/CorpusID:10600578}
}

@inproceedings{Lample2016NeuralAF,
  title     = {Neural Architectures for Named Entity Recognition},
  author    = {Lample, Guillaume  and
               Ballesteros, Miguel  and
               Subramanian, Sandeep  and
               Kawakami, Kazuya  and
               Dyer, Chris},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N16-1030},
  doi       = {10.18653/v1/N16-1030},
  pages     = {260--270}
}

@article{OpenAI2023GPT4TR,
  title   = {{GPT-4 Technical Report}},
  author  = {OpenAI},
  journal = {ArXiv},
  year    = {2023},
  volume  = {abs/2303.08774},
  url     = {https://api.semanticscholar.org/CorpusID:257532815}
}

@inproceedings{Kroeger1992PhraseSA,
  title  = {{Phrase Structure and Grammatical Relations in Tagalog}},
  author = {Paul R. Kroeger},
  year   = {1992},
  url    = {https://api.semanticscholar.org/CorpusID:60973484}
}

@online{DatabricksBlog2023DollyV2,
  author  = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title   = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
  year    = {2023},
  url     = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
  urldate = {2023-06-30}
}

@article{falcon40b,
  title  = {{Falcon-40B}: an open large language model with state-of-the-art performance},
  author = {Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year   = {2023}
}

@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title  = {OpenLLaMA: An Open Reproduction of LLaMA},
  month  = May,
  year   = 2023,
  url    = {https://github.com/openlm-research/open_llama}
}

@inproceedings{Strassel2016LORELEILP,
  title     = {{LORELEI} Language Packs: Data, Tools, and Resources for Technology Development in Low Resource Languages},
  author    = {Strassel, Stephanie  and
               Tracey, Jennifer},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)},
  month     = may,
  year      = {2016},
  address   = {Portoro{\v{z}}, Slovenia},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/L16-1521},
  pages     = {3273--3280},
  abstract  = {In this paper, we describe the textual linguistic resources in nearly 3 dozen languages being produced by Linguistic Data Consortium for DARPA{'}s LORELEI (Low Resource Languages for Emergent Incidents) Program. The goal of LORELEI is to improve the performance of human language technologies for low-resource languages and enable rapid re-training of such technologies for new languages, with a focus on the use case of deployment of resources in sudden emergencies such as natural disasters. Representative languages have been selected to provide broad typological coverage for training, and surprise incident languages for testing will be selected over the course of the program. Our approach treats the full set of language packs as a coherent whole, maintaining LORELEI-wide specifications, tagsets, and guidelines, while allowing for adaptation to the specific needs created by each language. Each representative language corpus, therefore, both stands on its own as a resource for the specific language and forms part of a large multilingual resource for broader cross-language technology development.}
}

@inproceedings{Papay2020DissectingSI,
  title     = {Dissecting Span Identification Tasks with Performance Prediction},
  author    = {Papay, Sean  and
               Klinger, Roman  and
               Pad{\'o}, Sebastian},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.396},
  doi       = {10.18653/v1/2020.emnlp-main.396},
  pages     = {4881--4895},
  abstract  = {Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks{'} properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.}
}

@inproceedings{Doddington2004TheAC,
  title     = {The Automatic Content Extraction ({ACE}) Program {--} Tasks, Data, and Evaluation},
  author    = {Doddington, George  and
               Mitchell, Alexis  and
               Przybocki, Mark  and
               Ramshaw, Lance  and
               Strassel, Stephanie  and
               Weischedel, Ralph},
  booktitle = {Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04)},
  month     = may,
  year      = {2004},
  address   = {Lisbon, Portugal},
  publisher = {European Language Resources Association (ELRA)},
  url       = {http://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf}
}

@article{Schachter1973TagalogRG,
  title   = {{Tagalog reference grammar}},
  author  = {Paul Schachter and Fe T. Otanes},
  journal = {The Journal of Asian Studies},
  year    = {1973},
  volume  = {32},
  pages   = {760 - 761},
  url     = {https://api.semanticscholar.org/CorpusID:162393304}
}

@inproceedings{Pan2017CrosslingualNT,
  title     = {Cross-lingual Name Tagging and Linking for 282 Languages},
  author    = {Pan, Xiaoman  and
               Zhang, Boliang  and
               May, Jonathan  and
               Nothman, Joel  and
               Knight, Kevin  and
               Ji, Heng},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P17-1178},
  doi       = {10.18653/v1/P17-1178},
  pages     = {1946--1958},
  abstract  = {The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating {``}silver-standard{''} annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.}
}

@article{Costiniano2022CustomCG,
  title   = {{Custom Coarse Grained Named Entity Recognition for Filipino Storytelling Data Using Uncased Transformer Models}},
  author  = {Sherwyne Costiniano and Rose Ann Mae Santos and Julius Simon Mendoza and Allen Jay Gale},
  journal = {SSRN Electronic Journal},
  year    = {2022},
  url     = {https://api.semanticscholar.org/CorpusID:255117774}
}


@misc{Lewis2009EthnologueL,
  title        = {{Ethnologue: languages of the world}},
  author       = {Paul M. A. Lewis},
  howpublished = {\url{https://ethnologue.com/language/tgl}},
  year         = {2009},
  note         = {Accessed: June 2023}
}


@misc{Reiter2017HT,
  title        = {{How to Develop Annotation Guidelines}},
  author       = {Nils Reiter},
  howpublished = {\url{https://sharedtasksinthedh.github.io/2017/10/01/howto-annotation/}},
  year         = {2017},
  note         = {Accessed: June 2023}
}


@inproceedings{Pires2019HowMI,
  title     = {{How Multilingual is Multilingual BERT?}},
  author    = {Telmo Pires and Eva Schlinger and Dan Garrette},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2019}
}

@inproceedings{Devlin2019BERTPO,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{Conneau2019UnsupervisedCR,
  title     = {{Unsupervised Cross-lingual Representation Learning at Scale}},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}


@inproceedings{Cruz2021ImprovingLL,
  title     = {{Improving Large-scale Language Models and Resources for {F}ilipino}},
  author    = {Cruz, Jan Christian Blaise  and
               Cheng, Charibeth},
  booktitle = {Proceedings of the Thirteenth Language Resources and Evaluation Conference},
  month     = jun,
  year      = {2022},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2022.lrec-1.703},
  pages     = {6548--6555},
  abstract  = {In this paper, we improve on existing language resources for the low-resource Filipino language in two ways. First, we outline the construction of the TLUnified dataset, a large-scale pretraining corpus that serves as an improvement over smaller existing pretraining datasets for the language in terms of scale and topic variety. Second, we pretrain new Transformer language models following the RoBERTa pretraining technique to supplant existing models trained with small corpora. Our new RoBERTa models show significant improvements over existing Filipino models in three benchmark datasets with an average gain of 4.47{\%} test accuracy across three classification tasks with varying difficulty.}
}

@inproceedings{Aquino2020ParsingIT,
  title     = {{Parsing in the absence of related languages: Evaluating low-resource dependency parsers on Tagalog}},
  author    = {Angelina A. Aquino and Franz A. de Leon},
  booktitle = {Universal Dependencies Workshop},
  year      = {2020}
}

@inproceedings{Kondratyuk201975L1,
  title     = {{75 Languages, 1 Model: Parsing Universal Dependencies Universally}},
  author    = {D. Kondratyuk},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2019}
}

@inproceedings{Dehouck2019PhylogenicMD,
  title     = {{Phylogenic Multi-Lingual Dependency Parsing}},
  author    = {Mathieu Dehouck and P. Denis},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  year      = {2019}
}

@article{Enriquez2023DeterminingLF,
  title   = {{Determining Linguistic Features of Hate Speech from 2016 Philippine Election-Related Tweets}},
  author  = {Raphael Christen K. Enriquez and Maria Regina Justina Estuar},
  journal = {2023 International Conference on IT Innovation and Knowledge Discovery (ITIKD)},
  year    = {2023},
  pages   = {1-6}
}

@article{Cabasag2016HatespeechIP,
  title   = {{Hate Speech in Philippine Election-Related Tweets: Automatic Detection and Classification Using Natural Language Processing}},
  author  = {Neil Vicente P. Cabasag and Vicente Raphael C. Chan and Sean Christian Y. Lim and Mark Edward M. Gonzales and Charibeth K. Cheng},
  journal = {Philippine Computing Journal Dedicated Issue on Natural Language Processing},
  year    = {2019},
  pages   = {1-14}
}

@article{Livelo2018IntelligentDI,
  title   = {{Intelligent Dengue Infoveillance Using Gated Recurrent Neural Learning and Cross-Label Frequencies}},
  author  = {Evan Dennison S. Livelo and Charibeth Ko Cheng},
  journal = {2018 IEEE International Conference on Agents (ICA)},
  year    = {2018},
  pages   = {2-7}
}

@article{Honnibal2020Spacy,
  title  = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  doi    = {10.5281/zenodo.1212303},
  year   = {2020}
}

@inproceedings{Enevoldsen2021DaCyAU,
  title     = {{DaCy: A Unified Framework for Danish NLP}},
  author    = {Kenneth C. Enevoldsen and L M Hansen and Kristoffer Laigaard Nielbo},
  booktitle = {Workshop on Computational Humanities Research},
  year      = {2021}
}

@article{Orosz2022HuSpaCyAI,
  title   = {{HuSpaCy: an industrial-strength Hungarian natural language processing toolkit}},
  author  = {Gy{\"o}rgy Orosz and Zsolt Sz{\'a}nt{\'o} and P{\'e}ter Berkecz and Gergo Szab{\'o} and Rich{\'a}rd Farkas},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2201.01956}
}

@article{Eyre2021LaunchingIC,
  title   = {{Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python}},
  author  = {Hannah Eyre and Alec B. Chapman and Kelly S. Peterson and Jianlin Shi and Patrick R. Alba and Makoto M. Jones and Tam{\'a}ra L Box and Scott L Duvall and Olga V Patterson},
  journal = {Proceedings of the AMIA Annual Symposium},
  year    = {2021},
  volume  = {2021},
  pages   = {438-447}
}

@article{Neumann2019ScispaCyFA,
  title   = {{ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing}},
  author  = {Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1902.07669}
}

@article{Wolf2019HuggingFacesTS,
  title   = {{HuggingFace's Transformers: State-of-the-art Natural Language Processing}},
  author  = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Jamie Brew},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1910.03771}
}

@inproceedings{Cruz2020ExploitingNA,
  title     = {{Exploiting News Article Structure for Automatic Corpus Generation of Entailment Datasets}},
  author    = {Jan Christian Blaise Cruz and Jose Kristian Resabal and James Lin and Dan John Velasco and Charibeth Ko Cheng},
  booktitle = {Pacific Rim International Conference on Artificial Intelligence},
  year      = {2020}
}

@inproceedings{OrtizSuarez2019AsynchronousPF,
  title     = {{Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures}},
  author    = {Pedro Ortiz Suarez and Beno{\^i}t Sagot and Laurent Romary},
  booktitle = {7th Workshop on the Challenges in the Management of Large Corpora},
  year      = {2019}
}

@inproceedings{Sang2003IntroductionTT,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}

@inproceedings{Sang2002IntroductionTT,
  title     = {Introduction to the {C}o{NLL}-2002 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.},
  booktitle = {{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002)},
  year      = {2002},
  url       = {https://aclanthology.org/W02-2024}
}

@article{Delger2012BuildingGS,
  title   = {{Building Gold Standard Corpora for Medical Natural Language Processing Tasks}},
  author  = {Louise Del{\'e}ger and Qi Li and Todd Lingren and Megan Kaiser and Katalin Moln{\'a}r and Laura Stoutenborough and Michal Kouril and Keith A. Marsolo and Imre Solti},
  journal = {Proceedings of the AMIA Annual Symposium},
  year    = {2012},
  volume  = {2012},
  pages   = {144-53}
}

@article{Bojanowski2016EnrichingWV,
  title     = {{Enriching Word Vectors with Subword Information}},
  author    = {Bojanowski, Piotr  and
               Grave, Edouard  and
               Joulin, Armand  and
               Mikolov, Tomas},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {5},
  year      = {2017},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/Q17-1010},
  doi       = {10.1162/tacl_a_00051},
  pages     = {135--146},
  abstract  = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.}
}


@inproceedings{Vaswani2017AttentionIA,
  title     = {{Attention is All you Need}},
  author    = {Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle = {Proceedings of the Conference on Neural Information Processing Systems},
  year      = {2017}
}

@inproceedings{Jiang2021PretrainedLM,
  title     = {{Pre-trained Language Models for Tagalog with Multi-source Data}},
  author    = {Shengyi Jiang and Yingwen Fu and Xiaotian Lin and Nankai Lin},
  booktitle = {Natural Language Processing and Chinese Computing},
  year      = {2021}
}

@mastersthesis{Samson2018TRG,
  author  = {Stephanie Dawn Samson},
  type    = {Bachelor's Thesis},
  title   = {{A treebank prototype of Tagalog}},
  school  = {University of T{\"u}bingen},
  address = {Germany},
  year    = {2018}
}

@inproceedings{Agic2017CrossLingualPS,
  title     = {{Cross-Lingual Parser Selection for Low-Resource Languages}},
  author    = {Agi\'{c}, \v{Z}eljko},
  booktitle = {Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies},
  pages     = {1-10},
  year      = {2017}
}


@inproceedings{Haspelmath2005WALS,
  title     = {{The World Atlas of Language Structures}},
  author    = {Martin Haspelmath and Matthew Dryer and David Gil and Comrie Bernard},
  booktitle = {Oxford University Press},
  year      = {2005}
}

@article{Miranda2022MultiHE,
  title   = {{Multi hash embeddings in spaCy}},
  author  = {Lester James V. Miranda and \'Akos K\'ad\'ar and Adriane Boyd and Sofie Van Landeghem and Anders S{\o}gaard and Matthew Honnibal},
  journal = {ArXiv},
  year    = {2022},
  volume  = {abs/2212.09255}
}

@inproceedings{Brandsen2020CreatingAD,
  title     = {{Creating a Dataset for Named Entity Recognition in the Archaeology Domain}},
  author    = {Brandsen, Alex  and
               Verberne, Suzan  and
               Wansleeben, Milco  and
               Lambers, Karsten},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  month     = may,
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  url       = {https://aclanthology.org/2020.lrec-1.562},
  pages     = {4573--4577},
  abstract  = {In this paper, we present the development of a training dataset for Dutch Named Entity Recognition (NER) in the archaeology domain. This dataset was created as there is a dire need for semantic search within archaeology, in order to allow archaeologists to find structured information in collections of Dutch excavation reports, currently totalling around 60,000 (658 million words) and growing rapidly. To guide this search task, NER is needed. We created rigorous annotation guidelines in an iterative process, then instructed five archaeology students to annotate a number of documents. The resulting dataset contains {\textasciitilde}31k annotations between six entity types (artefact, time period, place, context, species {\&} material). The inter-annotator agreement is 0.95, and when we used this data for machine learning, we observed an increase in F1 score from 0.51 to 0.70 in comparison to a machine learning model trained on a dataset created in prior work. This indicates that the data is of high quality, and can confidently be used to train NER classifiers.},
  language  = {English},
  isbn      = {979-10-95546-34-4}
}



@inproceedings{Deleger2012BG,
  title     = {{Building gold standard corpora for medical natural language processing tasks}},
  author    = {Louise Deleger and Qi Li and Todd Lingren and Megan Kaiser and Katalin Molnar and Laura Stoutenborough and Michal Kouril and Keith Marsolo and Imre Solti},
  booktitle = {AMIA Annual Symposium Proceedings},
  year      = {2012},
  pages     = {144-53}
}

@misc{Nivre2020UniversalDV,
  title        = {{Data Release Checklist - Universal Dependencies}},
  author       = {Joakim Nivre and Marie-Catherine de Marneffe and Filip Ginter and Jan Hajivc and Christopher D. Manning and Sampo Pyysalo and Sebastian Schuster and Francis M. Tyers and Daniel Zeman},
  howpublished = {\url{https://universaldependencies.org/release_checklist.html#data-split}},
  year         = {2022},
  note         = {Accessed: June 2023}
}

@inproceedings{Lauscher2020FromZT,
  title     = {From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers},
  author    = {Anne Lauscher and Vinit Ravishankar and Ivan Vulic and Goran Glavas},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2020}
}

@article{Ruder2019ASO,
  title   = {A Survey of Cross-lingual Word Embedding Models},
  author  = {Sebastian Ruder and Ivan Vulic and Anders S{\o}gaard},
  journal = {J. Artif. Intell. Res.},
  year    = {2019},
  volume  = {65},
  pages   = {569-631}
}