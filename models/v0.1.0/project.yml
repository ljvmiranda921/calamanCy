title: "Release v0.1.0"
description: |
  This is a spaCy project that trains the v0.1.0 models for calamanCy. You can
  use this project to replicate the pipelines shipped by the project. First, you need to install the
  required dependencies:

  ```
  pip install -r requirements.txt
  ```

  Then you need to download all the assets (corpora, static vectors, etc.) via:

  ```
  python -m spacy project assets
  ```

  Before training the pipelines, you first need to run the setup workflow. This step prepares the
  corpora for training (or pretraining) and builds the necessary binaries for other downstream commands.

  ```
  python -m spacy project run setup
  ```

  You can then train a pipeline by passing its name to the spaCy project command. For example, if you wish to train
  `tl_calamancy_md`, you can execute the corresponding workflow like so:

  ```
  python -m spacy project run tl-calamancy-md 
  ```

vars:
  # Versioning
  version: v0.1.0
  dataset_version: 1.0
  pretrain_epochs: 3
  # Training
  lang: "tl"
  gpu_id: 0

remotes:
  # Create a service account, download a JSON key, and set the credentials path
  # to GOOGLE_APPLICATION_CREDENTIALS env variable
  gcs: "gs://ljvmiranda/calamanCy/training_cache/${vars.version}/"

directories:
  - "assets"
  - "assets/treebank/"
  - "configs"
  - "corpus"
  - "scripts"
  - "training"
  - "pretraining"
  - "vectors"

assets:
  - dest: assets/corpus.tar.gz
    description: "Annotated TLUnified corpora in spaCy format with train, dev, and test splits."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v${vars.dataset_version}/corpus.tar.gz"
  - dest: "assets/treebank/UD_Tagalog-Ugnayan/"
    description: "Treebank data for UD_Tagalog-Ugnayan"
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-Ugnayan"
      branch: "master"
      path: ""
  - dest: "assets/treebank/UD_Tagalog-TRG/"
    description: "Treebank data for UD_Tagalog-TRG"
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-TRG"
      branch: "master"
      path: ""
  - dest: "assets/fasttext.tl.gz"
    description: "Tagalog fastText vectors provided from the fastText website (trained from CommonCrawl and Wikipedia)."
    url: "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tl.300.vec.gz"
  - dest: "assets/tlunified.zip"
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/tlunified/tlunified.zip"
    description: "TLUnified dataset (from Improving Large-scale Language Models and Resources for Filipino by Cruz and Cheng 2022)."
  - dest: "assets/floret"
    description: "Floret repository for training floret and fastText models."
    git:
      repo: "https://github.com/explosion/floret"
      branch: "main"
      path: ""
  - dest: "assets/tlunified_raw_text.jsonl"
    description: "Pre-converted raw text from TLUnified in JSONL format (1.1 GB)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tlunified_raw_text.jsonl"

workflows:
  setup:
    - "setup-training-data"
    - "setup-pretraining-data"
    - "setup-fasttext-vectors"
    - "build-floret"
  tl-calamancy-trf:
    - "train-parser-tagger"
    - "train-ner-trf"
    - "assemble-trf"
  tl-calamancy-lg:
    - "pretrain-lg"
    - "train-vectors-lg"
    - "train-parser-tagger"
    - "train-ner-lg"
    - "assemble-lg"
  tl-calamancy-md:
    - "pretrain-md"
    - "train-vectors-md"
    - "train-parser-tagger"
    - "train-ner-md"
    - "assemble-md"

commands:
  - name: "setup-training-data"
    help: "Prepare the Tagalog corpora used for training various spaCy components"
    script:
      # ner: Extract Tagalog corpora
      - mkdir -p corpus/ner
      - "tar -xzvf assets/corpus.tar.gz -C corpus/ner"
      # parser, tagger: Convert treebank into spaCy format
      # we are merging these two treebanks because the amount of data is too small
      - mkdir -p corpus/treebank/
      - >-
        python -m spacy convert 
        assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu assets/treebank
        --converter conllu
        --n-sents 1
        --merge-subtokens
      - >-
        python -m spacy convert 
        assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu assets/treebank
        --converter conllu
        --n-sents 1
        --merge-subtokens
      - >-
        python -m scripts.merge_treebanks
        assets/treebank/tl_trg-ud-test.spacy assets/treebank/tl_ugnayan-ud-test.spacy
        assets/treebank/ud_merged.spacy
      - >-
        python -m scripts.split_treebank
        assets/treebank/ud_merged.spacy
        corpus/treebank
        --lang ${vars.lang}
        --train-size 0.9
    deps:
      - assets/corpus.tar.gz
      - assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu
      - assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu
    outputs:
      - corpus/ner/train.spacy
      - corpus/ner/dev.spacy
      - corpus/ner/test.spacy
      - corpus/treebank/train.spacy
      - corpus/treebank/dev.spacy

  - name: "setup-pretraining-data"
    help: "Prepare the Tagalog corpora used for self-supervised learning operations"
    script:
      - unzip -o assets/tlunified.zip -d assets/
    outputs:
      - assets/tlunified/train.txt

  - name: "setup-fasttext-vectors"
    help: "Make fastText vectors spaCy compatible"
    script:
      - gzip -d -f assets/fasttext.tl.gz
      - mkdir -p vectors/fasttext-tl
      - >-
        python -m spacy init vectors
        tl assets/fasttext.tl vectors/fasttext-tl
    deps:
      - assets/fasttext.tl.gz
    outputs:
      - vectors/fasttext-tl

  - name: "build-floret"
    help: "Build floret binary for training fastText / floret vectors"
    script:
      - make -C assets/floret
      - chmod +x assets/floret/floret
    deps:
      - assets/floret
    outputs:
      - assets/floret/floret

  - name: "train-vectors-md"
    help: "Train medium-sized word vectors (200 dims, 50k keys) using the floret binary."
    script:
      - mkdir -p assets/vectors/floret-tl-md/
      - >-
        assets/floret/floret skipgram
        -input assets/tlunified/train.txt 
        -output assets/vectors/floret-tl-md/vectors
        -dim 200
        -minn 3
        -maxn 5 
        -mode floret
        -hashCount 2
        -bucket 50000
      - mkdir -p vectors/floret-tl-md
      - >-
        python -m spacy init vectors 
        tl assets/vectors/floret-tl-md/vectors.floret vectors/floret-tl-md
        --mode floret
    deps:
      - assets/floret/floret
    outputs:
      - vectors/floret-tl-md

  - name: "train-vectors-lg"
    help: "Train large-sized word vectors (200 dims, 200k keys) using the floret binary."
    script:
      - mkdir -p assets/vectors/floret-tl-lg/
      - >-
        assets/floret/floret skipgram
        -input assets/tlunified/train.txt 
        -output assets/vectors/floret-tl-lg/vectors
        -dim 200
        -minn 3
        -maxn 5 
        -mode floret
        -hashCount 2
        -bucket 200000
      - mkdir -p vectors/floret-tl-lg
      - >-
        python -m spacy init vectors 
        tl assets/vectors/floret-tl-lg/vectors.floret vectors/floret-tl-lg
        --mode floret
    deps:
      - assets/floret/floret
    outputs:
      - vectors/floret-tl-lg

  - name: "pretrain-md"
    help: "Pretrain with information from raw text using floret (md) vectors"
    script:
      - >-
        python -m spacy pretrain configs/ner.cfg pretraining/init-tok2vec-md/
        --paths.raw_text assets/tlunified_raw_text.jsonl
        --pretraining.max_epochs ${vars.pretrain_epochs}
        --pretraining.n_save_every 1
        --paths.vectors vectors/floret-tl-md
        --gpu-id ${vars.gpu_id}
    deps:
      - assets/tlunified_raw_text.jsonl
      - vectors/floret-tl-md
    outputs:
      - pretraining/init-tok2vec-md/model-last.bin

  - name: "pretrain-lg"
    help: "Pretrain with information from raw text using fastText vectors"
    script:
      - >-
        python -m spacy pretrain configs/ner.cfg pretraining/init-tok2vec-lg/
        --paths.raw_text assets/tlunified_raw_text.jsonl
        --pretraining.max_epochs ${vars.pretrain_epochs}
        --pretraining.n_save_every 1
        --paths.vectors vectors/fasttext-tl
        --gpu-id ${vars.gpu_id}
    deps:
      - assets/tlunified_raw_text.jsonl
      - vectors/fasttext-tl
    outputs:
      - pretraining/init-tok2vec-lg/model-last.bin

  - name: "train-parser-tagger"
    help: "Train the parser and tagger components using the Universal Dependencies Ugnayan Treebank"
    script:
      - >-
        python -m spacy train
        configs/parser_tagger.cfg
        --output training/parser_tagger/
        --nlp.lang ${vars.lang}
        --paths.train corpus/treebank/train.spacy
        --paths.dev corpus/treebank/dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/treebank/train.spacy
      - corpus/treebank/dev.spacy
    outputs:
      - training/parser_tagger/model-best

  - name: "train-ner-md"
    help: "Train NER component of tl_calamancy_md using floret vectors with pretraining (50k unique vectors)"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --nlp.lang ${vars.lang}
        --output training/ner_md/
        --paths.train corpus/ner/train.spacy
        --paths.dev corpus/ner/dev.spacy
        --initialize.init_tok2vec pretraining/init-tok2vec-md/model-last.bin
        --paths.vectors vectors/floret-tl-md 
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/ner/train.spacy
      - corpus/ner/dev.spacy
      - pretraining/init-tok2vec-md/model-last.bin
      - vectors/floret-tl-md
    outputs:
      - training/ner_md/model-best

  - name: "train-ner-lg"
    help: "Train NER component of tl_calamancy_lg using fastText vectors with pretraining (714k unique keys)"
    script:
      - >-
        python -m spacy train
        configs/ner.cfg
        --nlp.lang ${vars.lang}
        --output training/ner_lg/
        --paths.train corpus/ner/train.spacy
        --paths.dev corpus/ner/dev.spacy
        --initialize.init_tok2vec pretraining/init-tok2vec-lg/model-last.bin
        --paths.vectors vectors/fasttext-tl
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/ner/train.spacy
      - corpus/ner/dev.spacy
      - assets/tl_tlunified_pt_chars.bin
      - pretraining/init-tok2vec-lg/model-last.bin
      - vectors/fasttext-tl
    outputs:
      - training/ner_lg/model-best

  - name: "train-ner-trf"
    help: "Train NER component of tl_calamancy_trf using context-sensitive vectors from roberta-tagalog"
    script:
      - >-
        python -m spacy train
        configs/ner_trf.cfg
        --nlp.lang ${vars.lang}
        --output training/ner_trf/
        --components.transformer.model.name jcblaise/roberta-tagalog-base 
        --paths.train corpus/ner/train.spacy
        --paths.dev corpus/ner/dev.spacy
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/ner/train.spacy
      - corpus/ner/dev.spacy
    outputs:
      - training/ner_trf/model-best

  - name: "assemble-trf"
    help: "Assemble the tl_calamancy_trf model and package it as a spaCy pipeline"
    script:
      - ls

  - name: "assemble-lg"
    help: "Assemble the tl_calamancy_lg model and package it as a spaCy pipeline"
    script:
      - ls

  - name: "assemble-md"
    help: "Assemble the tl_calamancy_md model and package it as a spaCy pipeline"
    script:
      - ls
