title: "Benchmarking gold-annotated TLUnified data"
description: |
  This project performs structured evaluation for a gold-annotated Tagalog
  dataset. I first annotated the silver-standard annotations from WikiANN to
  produce gold-standard data. After three months of annotation, I obtained 7k+
  samples with Person (PER), Organization (ORG), and Location (LOC) entities:

  | Tagalog Data    | Documents | Tokens | PER  | ORG  | LOC  |
  |-----------------|-----------|--------|------|------|------|
  | Training Set    | 6252      | 198588 | 6418 | 3121 | 3296 |
  | Development Set | 782       |  25007 |  793 |  392 |  409 |
  | Test Set        | 782       |  25153 |  818 |  423 |  438 |

  Then, I benchmarked word vector and transformer-based spaCy pipelines in
  different settings (e.g., with or without pretraining, fastText/floret static
  vectors, etc.).

  Finally, I came up with a (1) word vector-based pipeline that consists of
  [floret vectors](https://github.com/explosion/floret) and [tok2vec weight
  pretraining](https://spacy.io/usage/embeddings-transformers#pretraining)
  (`tl_tlunified_lg`) and a (2) transformer-based pipeline using
  [roberta-tagalog-large](https://huggingface.co/jcblaise/roberta-tagalog-large)
  (`tl_tlunified_trf`). The results are shown below:

  | Pipeline                 | Precision       | Recall          | F1-score        |
  |--------------------------|-----------------|-----------------|-----------------|
  | tl_tlunified_lg          | 0.85 (0.01)     | 0.86 (0.02)     | 0.86 (0.02)     |
  | tl_tlunified_trf (base)  | 0.87 (0.02)     | 0.87 (0.01)     | 0.87 (0.01)     |
  | tl_tlunified_trf (large) | 0.89 (0.01)     | 0.89 (0.00)     | 0.90 (0.02)     |

  You can find the full write-up in my blog post, [*Towards a Tagalog NLP
  Pipeline*](https://ljvmiranda921.github.io/notebook/2023/02/04/tagalog-pipeline/),
  where I discuss my annotation process, architectural decisions, and
  performance evaluation.

  ### Running a benchmark

  Instead of using spaCy commands, we use the `scripts/benchmark.py`
  command-line tool to run a benchmark. Under the hood, it calls the `ner` (or
  `ner-trf`) workflow to preprocess data, train a model, and evaluate results.

  ```sh
  python3 -m scripts.benchmark experiment_id [OPTIONS]
  ```

  For example, if we want to conduct a pipeline that uses fastText vectors for
  three trials, we can run the command:

  ```sh
  # Run benchmark on a pipeline that uses fastText vectors
  python -m scripts.benchmark with_fasttext --num-trials 3 --vectors vectors/fasttext-tl
  ```

  You can check the `scripts/run_benchmarks.sh` file to see different
  experiments that leverage this tool.

  ### Hyperparameter search

  You can also perform hyperparameter search using [spaCy's Weight and Biases
  integration](https://docs.wandb.ai/guides/integrations/spacy). First, you need to login:

  ```sh
  wandb login
  ```

  And then you can run the `hyperparameter-search` command:

  ```sh
  python -m spacy project run hyperparameter-search
  ```

  ### Training curve

  To run the `train-curve` command, you need to install your own version of [Prodigy](https://prodi.gy/buy).
  Here, I trained the model with different portions of the training examples and print the accuracy figures
  in a figure.

  ```sh
  python -m pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy
  ```

  With `XXXX-XXXX-XXXX-XXXX` being your personal Prodigy license key.

vars:
  # General variables
  dataset_version: 1.0
  pretrain_epoch: 5
  vectors_version: 1.0
  seed: 42
  gpu_id: 0
  lang: "tl"
  raw_text: "raw_text.jsonl"
  prodigy_dataset: "calamancy_gold"
  # Experiment related variables called by
  # the script/run_experiment.py script
  experiment_id: "baseline"
  eval_dataset: "dev"
  config: "ner_chars.cfg"
  init_tok2vec: null
  vectors: null
  trial_num: 0
  trf_model_name: "roberta-tagalog-base"
  # fastText or floret training variables
  # cf. https://fasttext.cc/docs/en/unsupervised-tutorial.html#advanced-readers-playing-with-the-parameters
  vectors_dim: 200
  vectors_minn: 3
  vectors_maxn: 5
  vectors_bucket_size: 200000
  # Hyperparameter search
  wandb_config: "sweep.yml"
  sweep_n_trials: 30
  sweep_id: null

directories:
  - "assets"
  - "configs"
  - "corpus"
  - "scripts"
  - "pretraining"
  - "training"
  - "vectors"
  - "sweep"
  - "metrics"

assets:
  - dest: assets/corpus.tar.gz
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v${vars.dataset_version}/corpus.tar.gz"
    description: "Annotated TLUnified corpora in spaCy format with train, dev, and test splits."
  - dest: "assets/tl_tlunified_pt_chars.bin"
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/pt/tl_tlunified_pt_chars_e${vars.pretrain_epoch}.bin"
    description: "Pretraining weights for Tagalog using spaCy's pretrain command (using 'characters' objective)."
  - dest: "assets/tl_tlunified_pt_vects.bin"
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/pt/tl_tlunified_pt_vects_e${vars.pretrain_epoch}.bin"
    description: "Pretraining weights for Tagalog using spaCy's pretrain command (using 'vectors' objective)."
  - dest: "assets/tl_tlunified_pt_chars_floret.bin"
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/pt/tl_tlunified_pt_chars_e${vars.pretrain_epoch}_floret.bin"
    description: "Pretraining weights for Tagalog using spaCy's pretrain command (using 'characters' objective) and floret vectors."
  - dest: "assets/vectors.tar.gz"
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/vectors/v${vars.vectors_version}/vectors.tar.gz"
    description: "spaCy-compatible fastText and floret vectors."
  - dest: "assets/fasttext.tl.gz"
    url: "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.tl.300.vec.gz"
    description: "Tagalog fastText vectors provided from the fastText website (trained from CommonCrawl and Wikipedia)."
    extra: True
  - dest: assets/tl_tlunified_gold_v${vars.dataset_version}.jsonl
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v${vars.dataset_version}/tl_tlunified_gold.jsonl"
    description: "Annotated TLUnified dataset."
    extra: True
  - dest: "assets/tlunified.zip"
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/tlunified/tlunified.zip"
    description: "TLUnified dataset (from Improving Large-scale Language Models and Resources for Filipino by Cruz and Cheng 2022)."
    extra: True
  - dest: "assets/tlunified_raw_text.jsonl"
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tlunified_raw_text.jsonl"
    description: "Pre-converted raw text from TLUnified in JSONL format (1.1 GB)."
    extra: True
  - dest: "assets/floret"
    git:
      repo: "https://github.com/explosion/floret"
      branch: "master"
      path: ""
    description: "Floret repository for training floret and fastText models."
    extra: True

workflows:
  ner-trf:
    - "setup-ner"
    - "train-ner-trf"
    - "evaluate-ner"
  ner:
    - "setup-ner"
    - "train-ner"
    - "evaluate-ner"
  vectors:
    - "build-floret"
    - "train-vectors"
    - "init-vectors"

commands:
  - name: "preprocess"
    help: "Preprocess the raw annotated data and convert into spaCy format."
    script:
      - >-
        python -m scripts.preprocess
        assets/tl_tlunified_gold_v${vars.dataset_version}.jsonl corpus/
        --seed ${vars.seed}
        --shuffle
    deps:
      - assets/tl_tlunified_gold_v${vars.dataset_version}.jsonl
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy

  - name: "pretrain"
    help: "Pretrain with information from raw text"
    script:
      - >-
        python -m spacy pretrain configs/${vars.config} pretraining/
        --paths.raw_text assets/tlunified_raw_text.jsonl
        --paths.vectors ${vars.vectors}
        --gpu-id ${vars.gpu_id}
    deps:
      - assets/tlunified_raw_text.jsonl
    outputs:
      - pretraining/

  - name: "build-floret"
    help: "Build floret binary for training fastText and floret vectors."
    script:
      - make -C assets/floret
      - chmod +x assets/floret/floret
      - assets/floret/floret
    deps:
      - assets/floret
    outputs:
      - assets/floret/floret

  - name: "train-vectors"
    help: "Train word vectors using the floret binary."
    script:
      - unzip -o assets/tlunified.zip -d assets/
      - mkdir -p assets/vectors/tl-fasttext-tlunified/
      - >-
        assets/floret/floret skipgram
        -input assets/tlunified/train.txt
        -output assets/vectors/tl-fasttext-tlunified/vectors
        -dim ${vars.vectors_dim}
        -minn ${vars.vectors_minn}
        -maxn ${vars.vectors_maxn}
      - mkdir -p assets/vectors/tl-floret-tlunified/
      - >-
        assets/floret/floret skipgram
        -input assets/tlunified/train.txt
        -output assets/vectors/tl-floret-tlunified/vectors
        -dim ${vars.vectors_dim}
        -minn ${vars.vectors_minn}
        -maxn ${vars.vectors_maxn}
        -mode floret
        -hashCount 2
        -bucket ${vars.vectors_bucket_size}
    deps:
      - assets/floret/floret
    outputs:
      - assets/vectors/tl-fasttext-tlunified/vectors.vec
      - assets/vectors/tl-floret-tlunified/vectors.floret

  - name: "init-vectors"
    help: "Initialize word vectors."
    script:
      - "gzip -d -f assets/fasttext.tl.gz"
      - >-
        python -m spacy init vectors tl
        assets/fasttext.tl
        vectors/fasttext-tl
      - >-
        python -m spacy init vectors tl
        assets/vectors/tl-fasttext-tlunified/vectors.vec
        vectors/fasttext-tl-tlunified
      - >-
        python -m spacy init vectors tl
        assets/vectors/tl-floret-tlunified/vectors.floret
        vectors/floret-tl-tlunified
        --mode floret
    deps:
      - assets/fasttext.tl.gz
      - assets/vectors/tl-fasttext-tlunified/vectors.vec
      - assets/vectors/tl-floret-tlunified/vectors.floret
    outputs:
      - vectors/fasttext-tl
      - vectors/fasttext-tl-tlunified
      - vectors/floret-tl-tlunified

  - name: "setup-ner"
    help: "Prepare the Tagalog NER corpus, vectors, and pretrained weights."
    script:
      - "tar -xzvf assets/corpus.tar.gz -C corpus/"
      - "tar -xzvf assets/vectors.tar.gz -C vectors/"
    deps:
      - assets/corpus.tar.gz
    outputs:
      - corpus/train.spacy
      - corpus/dev.spacy
      - corpus/test.spacy
      - vectors/fasttext-tl
      - vectors/fasttext-tl-tlunified
      - vectors/floret-tl-tlunified

  - name: "train-ner"
    help: "Train the NER model. Usually called within the `benchmark.py` script."
    script:
      - >-
        python -m spacy train
        configs/${vars.config}
        --nlp.lang ${vars.lang}
        --output training/${vars.experiment_id}/${vars.trial_num}/
        --paths.train corpus/train.spacy
        --paths.dev corpus/dev.spacy
        --initialize.init_tok2vec ${vars.init_tok2vec}
        --paths.vectors ${vars.vectors}
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy
    outputs:
      - training/${vars.experiment_id}/${vars.trial_num}/model-best
      - training/${vars.experiment_id}/${vars.trial_num}/model-last

  - name: "train-ner-trf"
    help: "Train a transformer NER model. Usually called within the `benchmark.py` script."
    script:
      - >-
        python -m spacy train
        configs/${vars.config}
        --nlp.lang ${vars.lang}
        --output training/${vars.experiment_id}/${vars.trial_num}/
        --components.transformer.model.name ${vars.trf_model_name}
        --paths.train corpus/train.spacy
        --paths.dev corpus/dev.spacy
        --paths.vectors ${vars.vectors}
        --system.seed ${vars.seed}
        --gpu-id ${vars.gpu_id}
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy
    outputs:
      - training/${vars.experiment_id}/${vars.trial_num}/model-best
      - training/${vars.experiment_id}/${vars.trial_num}/model-last

  - name: "evaluate-ner"
    help: "Evaluate NER model. Usually called within the `benchmark.py` script."
    script:
      - mkdir -p metrics/${vars.experiment_id}/${vars.trial_num}/
      - >-
        python -m spacy evaluate
        training/${vars.experiment_id}/${vars.trial_num}/model-best
        corpus/test.spacy
        --output metrics/${vars.experiment_id}/${vars.trial_num}/scores_test.json
        --gpu-id ${vars.gpu_id}
      - >-
        python -m spacy evaluate
        training/${vars.experiment_id}/${vars.trial_num}/model-best
        corpus/dev.spacy
        --output metrics/${vars.experiment_id}/${vars.trial_num}/scores_dev.json
        --gpu-id ${vars.gpu_id}
    deps:
      - training/${vars.experiment_id}/${vars.trial_num}/model-best
      - corpus/test.spacy
    outputs:
      - metrics/${vars.experiment_id}/${vars.trial_num}/scores_test.json
      - metrics/${vars.experiment_id}/${vars.trial_num}/scores_dev.json

  - name: "hyperparameter-search"
    help: "Perform hyperparameter search given a training configuration."
    script:
      - >-
        python -m scripts.wandb_sweep configs/${vars.config}
        --sweep-id ${vars.sweep_id}
        --wandb-config configs/sweeps/${vars.wandb_config}
        --num-trials ${vars.sweep_n_trials}
        --gpu-id ${vars.gpu_id}
        --nlp.lang ${vars.lang}
        --paths.train corpus/train.spacy
        --paths.dev corpus/dev.spacy
        --paths.vectors ${vars.vectors}
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy

  - name: "summarize-results"
    help: "Summarize results for a given experimental run."
    script:
      - mkdir -p summarized_metrics/${vars.experiment_id}/
      - >-
        python -m scripts.collate_results
        metrics/${vars.experiment_id}
        --dataset ${vars.eval_dataset}
        --output-path summarized_metrics/${vars.experiment_id}/results.json
        --per-entity-type

  - name: "train-curve"
    help: "Train a model at varying portions of the training data"
    script:
      # Create JSONL files to hydrate a prodigy dataset into
      - python -m scripts.convert_to_jsonl corpus/train.spacy corpus/train.jsonl
      - python -m scripts.convert_to_jsonl corpus/dev.spacy corpus/dev.jsonl
      # Hydrate the prodigy dataset
      - python -m prodigy db-in ${vars.prodigy_dataset} corpus/train.jsonl
      - python -m prodigy db-in ${vars.prodigy_dataset}_eval corpus/dev.jsonl
      # Run train-curve command
      - >-
        python -m prodigy train-curve
        --ner ${vars.prodigy_dataset},eval:${vars.prodigy_dataset}_eval
        --config configs/${vars.config}
        --gpu-id ${vars.gpu_id}
        --show-plot
    deps:
      - corpus/train.spacy
      - corpus/dev.spacy
    outputs:
      - corpus/train.jsonl
      - corpus/dev.jsonl

  - name: "clean-datasets"
    help: "Drop the Prodigy dataset that was automatically created during the train-curve command"
    script:
      - python -m prodigy drop ${vars.prodigy_dataset}
      - python -m prodigy drop ${vars.prodigy_dataset}_eval
