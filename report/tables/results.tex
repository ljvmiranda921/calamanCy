\documentclass[../report.tex]{subfiles}
\begin{document}
\begin{table*}[t]
\begin{tabular}{@{}p{4cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
\toprule
                           & \multicolumn{2}{c}{\textbf{Text categorization}} & \textbf{NER} & \multicolumn{2}{c}{\textbf{Dep. pars. \& POS tag.}}                         \\ 
\textbf{Model}             & Hatespeech (binary) & Dengue (multilabel) & TLUnified-NER & Merged UD, UAS~/~LAS & Merged UD, POS Acc. \\ \midrule 
\textit{Monolingual (Ours)}              \\
tl\_calamancy\_md          & 74.40$\pm$0.05 & 65.32$\pm$0.04 & 87.67$\pm$0.03 & 76.47~/~54.40 & 96.70\\
tl\_calamancy\_lg          & 75.62$\pm$0.02 & 68.42$\pm$0.01 & 88.90$\pm$0.01 & 82.13~/~70.32 & 97.20\\
tl\_calamancy\_trf         & \textbf{78.25$\pm$0.06} & \textbf{72.45$\pm$0.02} & \textbf{90.34$\pm$0.02} & \textbf{92.48~/~80.90} & \textbf{97.80} \\ \midrule
\textit{Cross-lingual transfer} \\
uk\_core\_news\_trf        & 75.24$\pm$0.03 & 65.57$\pm$0.01 & 51.11$\pm$0.02 & 54.77~/~37.68 & 82.86 \\
ro\_core\_news\_lg         & 69.01$\pm$0.01 & 59.10$\pm$0.01 & 02.01$\pm$0.00 & 84.65~/~65.30 & 82.80 \\
ca\_core\_news\_trf        & 70.01$\pm$0.02 & 59.42$\pm$0.03 & 14.58$\pm$0.02 & 91.17~/~79.30 & 83.09 \\ \midrule
\textit{Multilingual finetuning} \\
xlm-roberta-base          & 77.57$\pm$0.01 & 67.20$\pm$0.01 & 88.03$\pm$0.03 & 88.34~/~76.07 & 94.29 \\
bert-base-multilingual    & 76.40$\pm$0.02 & 71.07$\pm$0.04 & 87.40$\pm$0.02 & 90.79~/~78.52 & 95.30 \\
\bottomrule
\end{tabular}
\caption{
    Benchmark evaluation scores for monolingual, cross-lingual, and multilingual pipelines across a variety of tasks and datasets.
    We evaluated the text categorization and NER tasks across five trials, and then conducted 10-fold cross-validation for dependency parsing.
    F1-scores are reported on the text categorization and NER tasks.
}
\label{table:results}
\end{table*}
\end{document}