title: "Benchmarking project for calamanCy"
description: |
  This is a spaCy project that benchmarks calamanCy on a variety of tasks.
  You can use this project to reproduce the experiments in the write-up. First, 
  you need to install the required dependencies:

  ```
  pip install -r requirements.txt
  ```

  Then run the set-up commands:

  ```
  python -m spacy project assets
  python -m spacy project run setup
  ```

  This step downloads all the necessary datasets and models for benchmarking
  use. You can then run one of the [workflows](#-) below. They are organized by
  task and a dataset identifier.

directories:
  - "assets"
  - "configs"
  - "corpus"
  - "training"
  - "metrics"

vars:
  seed: 0
  gpu_id: 0
  lang: "tl"
  calamancy_model_hash: 55ef01a244f3ca77676de6ba5a2beea0ba3e0021

assets:
  - dest: "assets/treebank/UD_Tagalog-Ugnayan/"
    description: "Treebank data for UD_Tagalog-Ugnayan. Originally sourced from *Parsing in the absence of related languages: Evaluating low-resource dependency parsers in Tagalog* by Aquino and de Leon (2020)."
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-Ugnayan"
      branch: "master"
      path: ""
  - dest: "assets/treebank/UD_Tagalog-TRG/"
    description: "Treebank data for UD_Tagalog-TRG. Originally sourced from the thesis, *A treebank prototype for Tagalog*, at the University of TÃ¼bingen by Samson (2018)."
    git:
      repo: "https://github.com/UniversalDependencies/UD_Tagalog-TRG"
      branch: "master"
      path: ""
  - dest: "assets/hatespeech.tar.gz"
    description: "Contains 10k tweets with 4.2k testing and validation data labeled as hate speech or non-hate speech (text categorization). Based on *Monitoring dengue using Twitter and deep learning techniques* by Livelo and Cheng (2018)."
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/hatenonhate/hatespeech_raw.zip"
  - dest: "assets/dengue.tar.gz"
    description: "Contains tweets on dengue labeled with five different categories. Tweets can be categorized to multiple categories at the same time (multilabel text categorization). Based on *Hate speech in Philippine election-related tweets: Automatic detection and classification using natural language processing* by Cabasag et al. (2019)"
    url: "https://s3.us-east-2.amazonaws.com/blaisecruz.com/datasets/dengue/dengue_raw.zip"
  - dest: assets/calamancy_gold.tar.gz
    description: "Contains the annotated TLUnified corpora in spaCy format with PER, ORG, LOC as entity labels (named entity recognition). Annotated by three annotators with IAA (Cohen's Kappa) of 0.78. Corpora was based from *Improving Large-scale Language Models and Resources for Filipino* by Cruz and Cheng (2021)."
    url: "https://storage.googleapis.com/ljvmiranda/calamanCy/tl_tlunified_gold/v1.0/corpus.tar.gz"

workflows:
  setup:
    - "install-models"
    - "process-datasets"
  textcat-hatespeech:
    - "train-hatespeech"
    - "evaluate-hatespeech"
  textcat_multilabel-dengue:
    - "train-dengue"
    - "evaluate-dengue"
  ner-calamancy_gold:
    # No training here because the training data is already
    # part of the original pipeline.
    - "evaluate-calamancy"
  parser-ud:
    # Using 10-fold cross-validation as recommended by the UD framework
    - "evaluate-ud"

commands:
  - name: "install-models"
    help: "Install models in the spaCy workspace"
    script:
      - pip install https://huggingface.co/ljvmiranda921/tl_calamancy_md/resolve/${vars.calamancy_model_hash}/tl_calamancy_md-any-py3-none-any.whl
      - pip install https://huggingface.co/ljvmiranda921/tl_calamancy_lg/resolve/${vars.calamancy_model_hash}/tl_calamancy_lg-any-py3-none-any.whl
      - pip install https://huggingface.co/ljvmiranda921/tl_calamancy_trf/resolve/${vars.calamancy_model_hash}/tl_calamancy_trf-any-py3-none-any.whl

  - name: "process-datasets"
    help: "Process the datasets and convert them into spaCy format"
    script:
      # textcat: extract and process Hatespeech dataset
      - mkdir -p assets/hatespeech/
      - tar -xzvf assets/hatespeech.tar.gz -C assets/hatespeech/
      - python -m scripts.process_hatespeech assets/hatespeech/ corpus/textcat-hatespeech/
      # textcat_multilabel: extract and process Dengue dataset
      - mkdir -p assets/dengue/
      - tar -xzvf assets/dengue.tar.gz -C assets/dengue/
      - python -m scripts.process_dengue assets/dengue.tar.gz corpus/textcat_multilabel-dengue/
      # ner: extract calamancy-gold dataset.
      - mkdir -p corpus/ner-calamancy_gold/
      - tar -xzvf assets/calamancy_gold.tar.gz -C corpus/ner-calamancy_gold/
      # parser: convert treebank into spaCy format and then merge them
      - mkdir -p corpus/treebank/
      - python -m spacy convert assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu assets/treebank --converter conllu --n-sents 1 --merge-subtokens
      - python -m spacy convert assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu assets/treebank --converter conllu --n-sents 1 --merge-subtokens
      - python -m scripts.merge_treebanks assets/treebank/tl_trg-ud-test.spacy assets/treebank/tl_ugnayan-ud-test.spacy assets/treebank/ud_merged.spacy --seed ${vars.seed}
      - python -m scripts.split_treebank assets/treebank/ud_merged.spacy corpus/treebank --lang ${vars.lang} --train-size 0.9 --seed ${vars.seed}
    deps:
      - assets/hatespeech.tar.gz
      - assets/dengue.tar.gz
      - assets/calamancy_gold.tar.gz
      - assets/treebank/UD_Tagalog-Ugnayan/tl_ugnayan-ud-test.conllu
      - assets/treebank/UD_Tagalog-TRG/tl_trg-ud-test.conllu
    outputs:
      - corpus/textcat-hatespeech/train.spacy
      - corpus/textcat-hatespeech/dev.spacy
      - corpus/textcat-hatespeech/test.spacy
      - corpus/textcat_multilabel-dengue/train.spacy
      - corpus/textcat_multilabel-dengue/dev.spacy
      - corpus/textcat_multilabel-dengue/test.spacy
      - corpus/ner-calamancy_gold/train.spacy
      - corpus/ner-calamancy_gold/dev.spacy
      - corpus/ner-calamancy_gold/test.spacy
      - corpus/parser-ud/train.spacy
      - corpus/parser-ud/dev.spacy
      - corpus/parser-ud/test.spacy

  - name: "train-hatespeech"
    help: "Train binary textcat on Hatespeech dataset"
    script:
      - ls
    deps:
      - corpus/textcat-hatespeech/train.spacy
      - corpus/textcat-hatespeech/dev.spacy
    outputs:
      - training/textcat-hatespeech/model-best/

  - name: "evaluate-hatespeech"
    help: "Evaluate binary textcat on Hatespeech test data"
    script:
      - ls
    deps:
      - training/textcat-hatespeech/model-best/
      - corpus/textcat-hatespeech/test.spacy
    outputs:
      - metrics/textcat-hatespeech.json

  - name: "train-dengue"
    help: "Train multilabel textcat on Dengue dataset"
    script:
      - ls
    deps:
      - corpus/textcat_multilabel-dengue/train.spacy
      - corpus/textcat_multilabel-dengue/dev.spacy
    outputs:
      - training/textcat_multilabel-dengue/model-best/

  - name: "evaluate-dengue"
    help: "Evaluate multilabel textcat on Dengue test data"
    script:
      - ls
    deps:
      - training/textcat_multilabel-dengue/model-best/
      - corpus/textcat_multilabel-dengue/test.spacy
    outputs:
      - metrics/textcat_multilabel-dengue.json

  - name: "evaluate-calamancy"
    help: "Evaluate ner on calamanCy gold dev and test data"
    script:
      - ls
    deps:
      - corpus/ner-calamancy_gold/dev.spacy
      - corpus/ner-calamancy_gold/test.spacy
    outputs:
      - metrics/ner-calamancy_gold-dev.json
      - metrics/ner-calamancy_gold-test.json

  - name: "evaluate-ud"
    help: "Evaluate parser and tagger on the combined Tagalog treebanks"
    script:
      - ls
    deps:
      - corpus/parser-ud/train.spacy
      - corpus/parser-ud/dev.spacy
      - corpus/parser-ud/test.spacy
    outputs:
      - metrics/parser-ud.json
