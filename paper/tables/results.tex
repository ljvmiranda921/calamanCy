\documentclass[../emnlp2023.tex]{subfiles}
\begin{document}
\begin{table*}[t]
\begin{tabular}{@{}p{4cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
\toprule
                           & \multicolumn{2}{c}{\textbf{Text categorization}} & \textbf{NER} & \multicolumn{2}{c}{\textbf{Dep. pars. \& POS tag.}}                         \\ 
\textbf{Model}             & \textit{Hatespeech (binary)} & \textit{Dengue (multilabel)} & \textit{TLUnified-NER} & \textit{Merged UD, UAS~/~LAS} & \textit{Merged UD, POS Acc.} \\ \midrule 
\textit{Monolingual (Ours)}              \\
tl\_calamancy\_md          & 74.40$\pm$0.05 & 65.32$\pm$0.04 & 87.67$\pm$0.03 & 76.47~/~54.40 & 98.70\\
tl\_calamancy\_lg          & 75.62$\pm$0.02 & 68.42$\pm$0.01 & 88.90$\pm$0.01 & 82.13~/~60.32 & 99.99\\
tl\_calamancy\_trf         & \textbf{78.25$\pm$0.06} & \textbf{72.45$\pm$0.02} & \textbf{90.34$\pm$0.02} & \textbf{92.48~/~80.90} & \textbf{99.99} \\ \midrule
\textit{Cross-lingual transfer} \\
uk\_core\_news\_trf        & 75.24$\pm$0.03 & 65.57$\pm$0.01 & 51.11$\pm$0.02 & 54.77~/~37.68 & 82.86 \\
ro\_core\_news\_lg         & 70.01$\pm$0.01 & 59.10$\pm$0.01 & 00.01$\pm$0.00 & 84.65~/~65.30 & 82.80 \\
ca\_core\_news\_trf        & 72.01$\pm$0.02 & 61.42$\pm$0.03 & 14.58$\pm$0.02 & 91.17~/~79.30 & 83.09 \\ \midrule
\textit{Multilingual approach} \\
xlm-roberta-large \\
bert-base-multilingual \\
\bottomrule
\end{tabular}
\caption{
    Benchmark evaluation scores for monolingual, cross-lingual, and multilingual pipelines across a variety of tasks and datasets.
    We evaluated the text categorization and NER tasks across five trials, and then conducted 10-fold cross-validation for dependency parsing.
    F1-scores are reported on the text categorization and NER tasks.
}
\label{table:results}
\end{table*}
\end{document}