\documentclass[../emnlp2023.tex]{subfiles}
\begin{document}
\begin{table*}[t]
\begin{tabular}{@{}p{4cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}@{}}
\toprule
                           & \multicolumn{2}{c}{\textbf{Text categorization}} & \textbf{NER} & \multicolumn{2}{c}{\textbf{Dependency parsing}}                         \\ 
\textbf{Model}             & \textit{Hatespeech (binary)} & \textit{Dengue (multilabel)} & \textit{TLUnified-NER} & \textit{Merged UD, UAS} & \textit{Merged UD, LAS} \\ \midrule 
\textit{Monolingual (Ours)}              \\
tl\_calamancy\_md   & 74.40$\pm$0.05 & 65.32$\pm$0.04 & 87.67$\pm$0.03 & 76.47 & 54.40\\
tl\_calamancy\_lg   & 75.62$\pm$0.02 & 68.42$\pm$0.01 & 88.90$\pm$0.01 & 82.13 & 60.32\\
tl\_calamancy\_trf  & \textbf{78.25$\pm$0.06} & 72.45$\pm$0.02 & 90.34$\pm$0.02 & 92.48 & 80.92\\ \midrule
\textit{Cross-lingual transfer} \\
uk\_core\_news\_trf & 75.24$\pm$0.03 \\
ro\_core\_news\_lg  & 70.01$\pm$0.01 \\
ca\_core\_news\_trf & 72.01$\pm$0.02 \\ \midrule
\textit{Multilingual approach} \\
xlm-roberta-large \\
bert-base-multilingual \\
\bottomrule
\end{tabular}
\caption{
    Benchmark evaluation scores for monolingual, cross-lingual, and multilingual pipelines across a variety of tasks and datasets.
    We evaluated the text categorization and NER tasks across five trials, and then conducted 10-fold cross-validation for dependency parsing.
}
\label{table:results}
\end{table*}
\end{document}